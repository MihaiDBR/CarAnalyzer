# ============================================
# BACKEND - app/scrapers/autovit.py
# Scraper real pentru Autovit.ro
# ============================================

import asyncio
import re
from datetime import datetime, timedelta
from typing import List, Optional
from dataclasses import dataclass

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

@dataclass
class CarListing:
    """StructurƒÉ pentru un anun»õ de ma»ôinƒÉ"""
    source: str
    url: str
    price: float
    marca: str
    model: str
    an: int
    km: int
    combustibil: str
    locatie: str
    dotari: List[str]
    data_publicare: datetime
    imagini: List[str]
    descriere: str
    telefon: Optional[str] = None

class AutovitScraper:
    """Scraper pentru Autovit.ro"""
    
    BASE_URL = "https://www.autovit.ro"
    
    def __init__(self, headless: bool = True):
        """
        Ini»õializeazƒÉ scraper-ul
        
        Args:
            headless: DacƒÉ True, browser-ul ruleazƒÉ fƒÉrƒÉ UI
        """
        options = Options()
        if headless:
            options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        self.driver = webdriver.Chrome(options=options)
        self.driver.implicitly_wait(10)
    
    async def search_cars(
        self, 
        marca: str, 
        model: str,
        an_min: Optional[int] = None,
        an_max: Optional[int] = None
    ) -> List[CarListing]:
        """
        CautƒÉ ma»ôini pe Autovit
        
        Args:
            marca: Marca ma»ôinii
            model: Modelul ma»ôinii
            an_min: An minim (op»õional)
            an_max: An maxim (op»õional)
            
        Returns:
            Lista de anun»õuri gƒÉsite
        """
        # Construie»ôte URL de cƒÉutare
        search_url = f"{self.BASE_URL}/autoturisme/{marca.lower().replace(' ', '-')}/{model.lower().replace(' ', '-')}"
        
        # AdaugƒÉ filtre pentru an
        params = []
        if an_min:
            params.append(f"search%5Bfilter_float_year%3Afrom%5D={an_min}")
        if an_max:
            params.append(f"search%5Bfilter_float_year%3Ato%5D={an_max}")
        
        if params:
            search_url += "?" + "&".join(params)
        
        print(f"üîç Scraping Autovit: {search_url}")
        
        try:
            self.driver.get(search_url)
            await asyncio.sleep(2)  # A»ôteaptƒÉ √ÆncƒÉrcarea
            
            # Scroll pentru a √ÆncƒÉrca toate rezultatele (lazy loading)
            await self._scroll_to_bottom()
            
            # Parse HTML
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # GƒÉse»ôte toate anun»õurile
            listings = []
            articles = soup.find_all('article', {'data-testid': 'listing-ad'})
            
            print(f"‚úì GƒÉsite {len(articles)} anun»õuri pe Autovit")
            
            for article in articles:
                try:
                    listing = self._parse_listing(article, marca, model)
                    if listing:
                        listings.append(listing)
                except Exception as e:
                    print(f"‚ö† Eroare la parsare anun»õ: {e}")
                    continue
            
            return listings
            
        except Exception as e:
            print(f"‚ùå Eroare la scraping Autovit: {e}")
            return []
    
    async def _scroll_to_bottom(self):
        """Scroll la sf√¢r»ôitul paginii pentru a √ÆncƒÉrca toate rezultatele"""
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        
        while True:
            # Scroll down
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            await asyncio.sleep(1.5)
            
            # CalculeazƒÉ noua √ÆnƒÉl»õime
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            
            if new_height == last_height:
                break
                
            last_height = new_height
    
    def _parse_listing(self, article, marca: str, model: str) -> Optional[CarListing]:
        """
        ParseazƒÉ un anun»õ individual
        
        Args:
            article: Element BeautifulSoup
            marca: Marca cƒÉutatƒÉ
            model: Modelul cƒÉutat
            
        Returns:
            CarListing sau None dacƒÉ parsarea e»ôueazƒÉ
        """
        try:
            # Extract URL
            link = article.find('a', {'class': 'offer-title__link'})
            if not link:
                return None
            
            url = link.get('href', '')
            if not url.startswith('http'):
                url = self.BASE_URL + url
            
            # Extract pre»õ
            price_elem = article.find('span', {'class': 'offer-price__number'})
            if not price_elem:
                return None
            
            price_text = price_elem.text.strip()
            price_text = price_text.replace(' ', '').replace('EUR', '').replace('.', '').replace(',', '.')
            
            try:
                price = float(price_text)
            except:
                return None
            
            # Extract titlu
            title = link.text.strip()
            
            # Extract detalii (an, km, combustibil)
            details = article.find_all('li', {'class': 'offer-item__params-item'})
            
            an = None
            km = None
            combustibil = "Necunoscut"
            
            for detail in details:
                text = detail.text.strip()
                
                # IdentificƒÉ anul (4 cifre)
                if re.match(r'^\d{4}$', text):
                    an = int(text)
                
                # IdentificƒÉ kilometrii
                elif 'km' in text.lower():
                    km_text = ''.join(filter(str.isdigit, text))
                    if km_text:
                        km = int(km_text)
                
                # IdentificƒÉ combustibilul
                elif any(fuel in text.lower() for fuel in ['benzin', 'diesel', 'electric', 'hybrid', 'gpl']):
                    combustibil = text
            
            # Extract loca»õie
            location_elem = article.find('span', {'class': 'offer-item__location'})
            locatie = location_elem.text.strip() if location_elem else "NecunoscutƒÉ"
            
            # Extract data publicƒÉrii
            date_elem = article.find('span', {'class': 'offer-item__add-date'})
            data_publicare = self._parse_date(date_elem.text.strip()) if date_elem else datetime.now()
            
            # Extract imagini (prima imagine)
            imagini = []
            img_elem = article.find('img', {'class': 'offer-item__photo'})
            if img_elem and img_elem.get('src'):
                imagini.append(img_elem['src'])
            
            return CarListing(
                source="autovit",
                url=url,
                price=price,
                marca=marca.title(),
                model=model.title(),
                an=an or 0,
                km=km or 0,
                combustibil=combustibil,
                locatie=locatie,
                dotari=[],  # Se completeazƒÉ la scraping detaliat
                data_publicare=data_publicare,
                imagini=imagini,
                descriere=""
            )
            
        except Exception as e:
            print(f"Eroare parsare listing: {e}")
            return None
    
    def _parse_date(self, date_str: str) -> datetime:
        """
        ParseazƒÉ data de publicare din text
        
        Args:
            date_str: Text cu data (ex: "AstƒÉzi", "Ieri", "Acum 3 zile")
            
        Returns:
            Obiect datetime
        """
        date_str = date_str.lower()
        
        if 'astƒÉzi' in date_str or 'today' in date_str:
            return datetime.now()
        elif 'ieri' in date_str or 'yesterday' in date_str:
            return datetime.now() - timedelta(days=1)
        else:
            # √éncearcƒÉ sƒÉ extragƒÉ numƒÉrul de zile
            match = re.search(r'(\d+)', date_str)
            if match:
                days = int(match.group(1))
                return datetime.now() - timedelta(days=days)
        
        return datetime.now()
    
    async def get_listing_details(self, url: str) -> dict:
        """
        Ob»õine detalii complete despre un anun»õ
        
        Args:
            url: URL-ul anun»õului
            
        Returns:
            Dict cu dotƒÉri, imagini, descriere, telefon
        """
        try:
            self.driver.get(url)
            await asyncio.sleep(2)
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # Extract dotƒÉri
            dotari = []
            equipment_section = soup.find('div', {'class': 'offer-features'})
            if equipment_section:
                items = equipment_section.find_all('li')
                dotari = [item.text.strip() for item in items]
            
            # Extract imagini
            imagini = []
            image_gallery = soup.find_all('img', {'class': 'photo-item'})
            imagini = [img['src'] for img in image_gallery if img.get('src')]
            
            # Extract descriere
            descriere = ""
            desc_elem = soup.find('div', {'class': 'offer-description'})
            if desc_elem:
                descriere = desc_elem.text.strip()
            
            # Extract telefon (dacƒÉ e vizibil)
            telefon = None
            phone_elem = soup.find('a', {'class': 'phone-number'})
            if phone_elem:
                telefon = phone_elem.text.strip()
            
            return {
                'dotari': dotari,
                'imagini': imagini,
                'descriere': descriere,
                'telefon': telefon
            }
            
        except Exception as e:
            print(f"Eroare la ob»õinere detalii: {e}")
            return {
                'dotari': [],
                'imagini': [],
                'descriere': '',
                'telefon': None
            }
    
    def close(self):
        """√énchide browser-ul"""
        try:
            self.driver.quit()
        except:
            pass